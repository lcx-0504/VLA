# Paper-Pi05: a Vision-Language-Action Model with Open-World Generalization

[Paper arxiv](https://arxiv.org/abs/2504.16054)

[Local PDF](papers/Paper-Pi05.pdf)

## Abstract核心要点

**目前问题**：虽然VLA模型在端到端机器人控制上表现优异，但**开放世界泛化** (open-world generalization) 仍然是未解决的核心挑战。

**解决方案**：π0.5基于π0，使用**异构任务联合训练** (co-training) 实现广泛泛化。关键在于"**异构数据** (heterogeneous data)" - 来自不同类型、不同来源的任务数据。

**技术路线**：

- **多源数据融合**：多机器人数据 + 高层语义预测 + 网络数据等
- **混合多模态样本**：图像观察 + 语言指令 + 物体检测 + 语义子任务预测 + 底层动作  
- **知识迁移验证**：证明知识迁移对有效泛化的重要性

**突破性成果**：**首次实现**端到端学习驱动的机器人系统在全新家庭环境中执行长时程精细操作技能（如厨房和卧室清理）。

![alt text](<Pi0.5 Paper Notes.assets/image.png>)

## Introduction要点解析

### 问题本质：从"能力不足"到"泛化不足"

当前机器人的根本问题不是技术能力（敏捷性、灵活性），而是**泛化能力**。大多数VLA模型在实验室表现优秀，但一到真实世界就失效。

**重要观点**：机器人需要的不是更强的单项能力，而是在**多个抽象层面**同时泛化的能力。

### 泛化的三个层次（以厨房清理为例）

| 泛化层次     | 难度 | 例子                               | 解决方案                 |
| ------------ | ---- | ---------------------------------- | ------------------------ |
| **感知泛化** | 低   | 识别不同外观的盘子、刀具           | 足够的视觉多样性         |
| **技能泛化** | 中   | 用新的方式或序列组合已有动作       | 跨环境、跨任务的技能迁移 |
| **语义泛化** | 高   | 理解"哪个是晾衣架"、"该开哪个抽屉" | 常识知识和推理能力       |

**关键发现**：不同层次的泛化需要**不同类型的数据源**，无法通过单一数据类型解决。

### 人类学习的启发：多源知识融合

人类解决新问题的能力来自**知识的组合运用**：

- **间接知识**：书本、他人经验 → 对应网络数据的语义知识
- **类比迁移**：其他任务的经验 → 对应跨机器人平台的技能迁移  
- **直接经验**：目标任务练习 → 对应目标平台的直接数据

**重要发现**：**知识多样性 > 数据相关性**。97.6%的"不相关"数据通过合理配方能产生强泛化。

### VLA的技术突破：统一建模框架

传统方法的问题：异构数据源无法有效整合，需要设计复杂的多模型系统。

**VLA的突破**：**序列建模框架** (sequence modeling) 将所有模态（视觉、语言、动作）统一为token序列，使异构数据的联合训练成为可能。

**技术优势**：

1. **统一表示**：不同数据类型可以在同一模型中处理
2. **端到端优化**：避免多模型系统的累积误差
3. **知识共享**：不同数据源的知识可以相互促进

**设计哲学转变**：从"收集更多直接数据"到"设计更好的数据配方"。

## Related Work：π0.5的技术定位

### 现有工作的进展与局限

**通用机器人操作策略的发展**：

近期工作（BridgeData V2、Open X-Embodiment等）证明将训练数据从单任务扩展到多场景、多任务的多样化数据集能显著提高泛化能力。VLA模型（如RT-2、OpenVLA）通过微调预训练视觉-语言模型，能够利用网络规模预训练获得的语义知识，结合高表达力的动作解码机制（flow matching、diffusion、高级action tokenization），在真实世界执行复杂操作任务。

**根本局限**：尽管语言遵循能力impressive，现有VLA仍然主要在**与训练数据密切匹配的环境**中评估。即使一些研究表明简单技能（如抓取物体、开抽屉）可以通过在更广泛环境中收集数据来泛化，但将同样方法应用到更复杂的长时程任务（如清理厨房）仍然困难，通过暴力扩展收集数据来覆盖所有可能场景是不可行的。

### π0.5的技术突破点

| 技术方向                 | 现有方法局限                       | π0.5的创新                                       |
| ------------------------ | ---------------------------------- | ------------------------------------------------ |
| **非机器人数据联合训练** | 主要局限于VLM数据混合              | 设计更广泛的机器人相关监督源联合训练系统         |
| **高层推理与规划**       | 使用两个独立模型（VLM + 低层策略） | **同一模型**进行高低层推理，类似chain-of-thought |
| **开放世界泛化**         | 限制在基本原语，任务相对简单       | 长时多阶段任务（10-15分钟），在全新家庭环境      |
| **数据利用策略**         | 主要依赖目标任务相关数据           | 97.6%"不相关"数据通过co-training实现强泛化       |

### 非机器人数据联合训练的探索

许多先前工作尝试使用多样化的非机器人数据来改进机器人策略的泛化：

- **视觉编码器初始化**：从计算机视觉数据集初始化视觉编码器
- **VLM数据混合**：RT-2等展示了与VLM训练数据联合训练能改善泛化，特别是与新物体或未见过的场景背景交互时
- **现成规划器**：利用现成的任务规划器辅助机器人控制

**π0.5的超越**：超越了VLM数据联合训练，设计了一个系统来与**更广泛的机器人相关监督源**进行联合训练，包括其他机器人数据、高层语义子任务预测、语言指导演示等。实验表明这种特定的数据源组合能够让移动机器人在全新环境中执行复杂长时程行为。

### 高层推理与语言规划

许多工作表明用高层推理增强端到端策略可以显著改善长时程任务性能，特别是当高层子任务推理可以利用大型预训练LLM和VLM时。

**现有方法**：通常使用**两个独立模型** - VLM预测语义步骤，独立的低层策略执行这些步骤。

**π0.5的方法**：也使用两阶段推理（首先推理高层语义子任务，然后基于此预测动作），但使用**同一个模型**进行高层和低层推理，配方更接近chain-of-thought或test-time compute方法。关键区别是高层推理仍然以低于低层动作推理的频率运行。

**技术优势**：统一模型使得高低层推理能够共享内部表示，不仅通过文本还通过hidden states传递信息，避免了两模型系统的理解不一致和优化困难问题。

### 开放世界泛化的评估

**现有工作的局限**：

大多数机器人学习系统在与训练数据密切匹配的环境中评估。当机器人任务限制在更窄的基本原语集合（如抓取）时，一些允许任务特定假设的方法（如grasp prediction、model-based planning and control）已经显示能够广泛泛化甚至到全新家庭，但这些方法不容易泛化到通用机器人可能需要执行的全部任务范围。

近期大规模数据集工作显示简单的端到端学习任务能够泛化到新环境，但这些演示中的任务仍然相对简单，通常少于1分钟长度，成功率往往较低。

**π0.5的突破**：

展示π0.5能够执行长时多阶段任务，如把所有盘子放进水槽或把所有衣物从新卧室地板上拾起，同时泛化到**完全新的家庭环境**（entirely new homes）。这是首次端到端学习驱动的机器人系统在全新环境中执行如此复杂和长时程的操作技能（10-15分钟）。

## Preliminaries：VLA技术基础

### VLA的训练目标

VLA模型通过模仿学习训练，目标是最大化给定观察和语言指令下，动作序列的对数似然：

$$
\max_\theta \mathbb{E}_{(a_{t:t+H}, o_t, \ell) \sim D} \left[ \log \pi_\theta(a_{t:t+H} | o_t, \ell) \right]
$$

**公式含义**：

- **θ**：模型参数（神经网络的权重），这是训练的目标。
- **D**：机器人演示数据集，包含以下的三元组的数据。
- **数据三元组（下标）**：
  - $a_{t:t+H}$：动作序列（action chunk），从时刻t到t+H这一段时间内的连续动作（如50步×7维=350个数值）。
  - $o_t$：时刻t的观察（由对环境的外部感知以及对自身状态的内部感知组成，详见[观察的组成结构](#观察的组成结构)）
  - $ℓ$：语言指令
- **期望 E**：对所有训练数据求平均
- **log π_θ(...)**：模型预测该动作序列的对数概率。取对数在保持优化目标不变的同时把乘法变成加法，避免数值计算问题。

**公式整体含义**：我们想要训练一组模型的参数θ，使得模型对于给定数据集D中的所有三元组（动作序列、观察、语言指令），能根据当前的观察和语言指令，预测出正确动作序列的对数概率的平均值最大化。

**训练过程**：通过调整参数θ，让模型在所有训练数据上预测正确动作序列的平均对数概率最大化。实际训练时用随机梯度下降，每次采样一小批数据计算loss并更新参数。

### 观察的组成结构

观察 $o_t$ 不是单一数据，而是**外部感知 + 内部感知**的组合：

#### 1. 外部感知 (Exteroception)

多视角图像 $I_1^t, ..., I_n^t$：

- 通常3个摄像头（手腕视角、头部视角、外部视角，具体数量根据具体任务和机器人平台而定）
- 每张图像如224×224×3的RGB图像
- 提供环境、物体位置等信息

#### 2. 内部感知 (Proprioception)

本体感知状态 $q_t$：

- **关节角度**：每个关节当前的角度值
- **夹爪状态**：开合程度（0到1）
- **底盘状态**（移动机器人）：位置、朝向、速度

**为什么需要两者结合？**

- 只有图像：看到物体但不知道自己手臂在哪
- 只有本体感知：知道姿势但不知道周围环境
- 两者结合：能够规划"如何从当前姿势到达目标"

### VLA架构设计

**基本架构**：遵循现代视觉-语言模型设计

| 组件                   | 作用                       | 技术                            |
| ---------------------- | -------------------------- | ------------------------------- |
| **模态特定tokenizers** | 将不同模态转成token表示    | 图像patches、文本分词、动作编码 |
| **自回归transformer**  | 从输入tokens生成输出tokens | 标准transformer架构             |
| **权重初始化**         | 从预训练VLM初始化          | 利用网络规模的语义知识          |

**统一表示机制**：

不同模态的数据（图像、文本、动作）含义完全不同，无法直接拼接。解决方案是通过**Embedding层**将所有模态转成相同维度的向量：

| 模态     | 原始格式          | Encoder           | 统一表示  |
| -------- | ----------------- | ----------------- | --------- |
| 文本     | "pick" → ID 1024  | Embedding Matrix  | 768维向量 |
| 图像     | 16×16×3 像素patch | Linear Projection | 768维向量 |
| 本体感知 | [θ1, θ2, ..., θ7] | Linear Layer      | 768维向量 |
| 动作     | 离散ID或连续值    | Embedding/Linear  | 768维向量 |

**统一到相同维度后**，所有tokens可以拼接成一个序列输入transformer，通过Attention机制互相交互。

**自回归生成**：模型逐个token生成输出，每次生成依赖前面已生成的tokens，类似GPT生成文本的方式。

### 动作表示的两种方法

**为什么需要特殊的动作表示？**

动作是连续值（如关节角度0.523 rad），与离散的文本token不同。需要特殊方法将动作转成token或向量表示。

| 方法                     | 表示形式      | 训练         | 推理             | 精度             |
| ------------------------ | ------------- | ------------ | ---------------- | ---------------- |
| **离散化 (FAST)**        | 8个离散tokens | 快速、稳定   | 自回归解码（慢） | 中（有量化误差） |
| **连续 (Flow Matching)** | 350维连续向量 | 需要迭代训练 | 迭代去噪（10步） | 高               |

**π0.5的策略**：

- **Pre-training**：使用FAST离散tokens（训练效率高，和文本统一处理）
- **Post-training**：切换到Flow Matching（精度高，适合实时控制）

### FAST编码原理

**问题**：50步×7维 = 350个连续值，如何高效编码？

**FAST的解决方案**：利用**时间相关性**压缩

**时间相关性**：机器人动作是平滑的，相邻时刻的动作非常相似。就像视频压缩利用相邻帧的相似性，FAST利用相邻时刻动作的相似性。

**压缩类比**：动作的"关键帧"

```text
原始: 50步完整动作序列（350个数）
  ↓ Encoder
8个关键"模式"（8个离散tokens）
  ↓ Decoder  
重建: 50步完整动作序列
```

**技术实现**：Vector Quantization (VQ)

1. **学习Codebook（动作模式库）**：包含256个codes，每个code是一个抽象的动作特征向量
2. **编码**：将350维动作映射到8个code IDs
3. **解码**：从8个codes通过神经网络decoder重建350维动作

**Code的本质**：

类似Language Model中的token：

- 离散的符号（ID）
- 对应抽象的向量表示
- 含义通过神经网络学习
- 不是人类可直接解释的"指令"，而是网络内部的抽象编码

**压缩效果**：350个数 → 8个tokens，压缩比约44倍

| 优势                | 劣势                      |
| :------------------ | :------------------------ |
| 序列短，训练快速    | 有量化误差                |
| 和文本token统一处理 | 推理需要8步自回归（较慢） |

### Flow Matching for 连续动作（在Post-training阶段使用）

**核心思想**：学习从噪声到真实动作的"流动路径"

**数学表示**：

从噪声 $x_0 \sim \mathcal{N}(0,I)$ 到真实动作 $x_1 = a_{t:t+H}$ 建立线性插值路径：

$$
x_\tau = (1-\tau) \cdot x_0 + \tau \cdot x_1, \quad \tau \in [0,1]
$$

路径的速度场（方向）：

$$
v_\tau = \frac{dx}{d\tau} = x_1 - x_0
$$

**训练目标**：

训练神经网络 $v_\theta(x, \tau)$ 预测速度场：

$$
L = \mathbb{E}_{\tau, x_0, x_1} \left[ ||v_\theta(x_\tau, \tau) - (x_1 - x_0)||^2 \right]
$$

**推理过程（生成动作）**：

从纯噪声开始，迭代更新：

$$
x_{i+1} = x_i + v_\theta(x_i, \tau_i) \cdot \Delta\tau
$$

```text
Step 0: x = 随机噪声
Step 1: x = x + v_θ(x, 0.0) × 0.1
Step 2: x = x + v_θ(x, 0.1) × 0.1
...
Step 10: x = 最终清晰的动作
```

**只需10步迭代即可生成高质量动作，比Diffusion Model的50-1000步快得多！**

**Flow Matching vs Diffusion**：

| 特性     | Diffusion   | Flow Matching          |
| -------- | ----------- | ---------------------- |
| 路径类型 | 随机（SDE） | 确定性（ODE）          |
| 训练目标 | 预测噪声    | 预测速度场             |
| 推理步数 | 50-1000步   | 5-10步                 |
| 实时性   | 慢          | 快，适合50Hz机器人控制 |

**π0.5选择Flow Matching的原因**：推理快速，满足实时控制需求。

关于Diffusion Model和Flow Matching的更多细节，请参考：

**[笔记 - Denoising Diffusion Probabilistic Models Tutorial（扩散模型）](../Flow-Matching/papers/DDPM-Tutorial.pdf)**

**[笔记 - Flow Matching Explained - From Noise to Robot Actions（流匹配）](../Flow-Matching/papers/Flow-Matching-Explained.pdf)**

### Action Expert设计

**什么是Action Expert？**

一个专门的小型transformer（300M参数），负责Flow Matching的动作生成。

**架构设计**：

```text
输入tokens → 主Transformer (2B参数)
              ↓
      ┌──────┴──────┐
      ↓              ↓
   文本tokens    动作tokens
      ↓              ↓
  主模型处理    Action Expert处理
      ↓              ↓
   文本输出      连续动作输出
```

**Action Expert的输入**：

1. 主transformer的内部表示（语义理解）
2. 当前的部分去噪动作 $x_\tau$
3. 时间步 $\tau$

**Action Expert的输出**：

- 速度场 $v_\theta(x_\tau, \tau)$

**为什么需要单独的Action Expert？**

1. **专业化**：主模型理解语义，Action Expert专注精确动作生成
2. **效率**：Action Expert较小（300M vs 2B），Flow Matching需要迭代10次更经济
3. **模块化**：Pre-training不需要，Post-training时才加入

**类比Mixture of Experts (MoE)**：不同类型的tokens由不同的"专家"模块处理。

### Pre-training vs Post-training对比

| 阶段              | 动作表示          | 模型组件            | 训练特点   | 目的                       |
| ----------------- | ----------------- | ------------------- | ---------- | -------------------------- |
| **Pre-training**  | FAST离散tokens    | VLM + FAST Decoder  | 快速、稳定 | 学习异构数据，建立基础能力 |
| **Post-training** | Flow Matching连续 | VLM + Action Expert | 精确、实时 | 专门优化目标任务的精确控制 |

**设计哲学**：结合两者优势，Pre-training用离散表示快速学习，Post-training用连续表示实现精确控制。
