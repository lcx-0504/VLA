# VLA Learning Notes

## 1. Pi0 & Pi0.5

### 1.1 Blog-Pi05: A VLA with Open-World Generalization

[Blog Link (Suggest for reading)](https://www.physicalintelligence.company/blog/pi05)

[Local PDF (Only for backup)](../Paper/Blog-Pi05:%20A%20VLA%20with%20Open-World%20Generalization.pdf)

#### 研究的背景问题是什么？

机器人最大的问题不在于敏捷或者灵活性，而在于泛化。低层次的泛化是识别外观不同的物体，高层次的泛化是理解任务背后的语义和意图。这要求机器人有不仅要感知环境，还要有物理、视觉和语音上的泛化。

当前机器人数据多样性的不足导致了泛化不足的问题，导致现在大多数机器人必须在工厂这类有严格控制的场所应用，而无法在日常生活中大面积运用。

当前大多数VLA（包括pi0）都是在与训练密切相关的特定环境评估的，但是最新的pi0.5表现出了更好的泛化能力。他的目标并不是新技能或者是高度灵活性，而是在于泛化（例如在没训练过的新的家庭环境中进行清理）。

#### 我们如何解决这个问题？

##### 核心原理

**pi0.5的核心原理是在异构数据上进行联合训练。**

**异构数据 (Heterogeneous Data)**: 不同类型、不同来源的数据。不是都来自mobile robot，不是都是action数据，而是包括图片、文字、动作等各种形式。从而可以学到：

1. 物理执行技能（怎么抓、怎么移动、怎么放）
2. 理解语义上下文（“清理厨房”的任务需要收拾什么，往哪里收拾）
3. 推断任务高层结构（”铺床”要拿枕头、整理床单、放回枕头等步骤规划）
4. 从其他机器人迁移（其他机器人的操作数据也有用，跨实体迁移）

**联合训练 (Co-Training)**: 同时在这些不同数据上训练同一个模型。VLA可以联合训练的重要原因是他是从VLM派生的。二者都是sequence modeling，把不同模态都当成token序列处理。

| 模型类型 | 输入类型                  |
| -------- | ------------------------- |
| VLM      | 图像 + 文字               |
| VLA      | 图像 + 文字 + 动作 + 其他 |

##### 具体的训练任务

1. 通用多模态任务（视觉理解和语义知识）
   - Image captioning (图像字幕): 图片 → "a dog playing frisbee"
   - Visual QA (视觉问答): 图片 + "How many desks?" → "12"
   - Object detection (对象检测): 图片 → bounding boxes

2. 机器人导向任务
   - **带动作的演示数据**：输入是图像+语言指令（如"put the cup in the sink"），输出是动作序列[a1, a2, a3, ...]
   - **高层次机器人示例**（重要！）：输入是观察到的场景（如unmade bed的图片），输出是语义层面的标签（如"pick up the pillow"）。注意这里输出的不是底层动作，而是语义指令
   - **语言指导演示**（Verbal Instruction）：人类像教练一样一步步告诉机器人做什么（例如："close the microwave" → 机器人执行 → "now pick up the mitten"），教会模型如何分解复杂任务和理解自然语言指令

##### 分层推理机制

| 推理层级   | 输入                | 输出                         | 类比                 | 从哪学习                    |
| ---------- | ------------------- | ---------------------------- | -------------------- | --------------------------- |
| High-Level | 当前场景 + 总任务   | 下一个语义步骤               | Chain-of-thought推理 | Web数据、语言指令、高层标注 |
| Low-Level  | 语义步骤 + 当前观察 | 电机命令（关节角度、速度等） | 具体动作执行         | 机器人动作数据              |

**例子**：High-level推理 "clean the kitchen" → "pick up the plate"，Low-level推理 "pick up the plate" → [θ1=0.5, θ2=-0.3, θ3=1.2, ...]

**分层的好处**：高层和低层可以从不同数据源学习，高层受益于语义知识（web数据），低层受益于物理技能（机器人数据），并且可以跨机器人平台迁移。

#### 实验设计与结果

##### 消融实验设计

训练VLA需要"正确的数据混合配方"，就像教人新工作需要合适的课程安排（conceptual + practical）。实验设计了5个版本，每个版本去掉一部分数据，验证各种数据的作用。

**数据来源**：

| 数据类型                 | 缩写 | 具体内容                           | 数据量    | 教会模型什么                 |
| ------------------------ | ---- | ---------------------------------- | --------- | ---------------------------- |
| **Web Data**             | WD   | 图像字幕、视觉问答、物体检测       | 大量      | 视觉语义理解、物体识别、常识 |
| **Multiple Environment** | ME   | 静态机器人在多个不同家庭的数据     | 中量      | 环境多样性、不同场景的物体   |
| **Cross Embodiment**     | CE   | π0原始数据集（不同类型的机器人）   | 中量      | 物理技能的跨平台迁移         |
| **Mobile Manipulation**  | -    | 目标平台的直接数据（mobile robot） | 约400小时 | 特定任务的直接经验           |

**实验模型版本**：

| 版本              | 去掉的数据         | 保留的数据            | 验证什么               |
| ----------------- | ------------------ | --------------------- | ---------------------- |
| **π0.5 (完整版)** | 无                 | WD + ME + CE + Mobile | 完整性能基准           |
| **no WD**         | Web数据            | ME + CE + Mobile      | Web数据是否必要？      |
| **no ME**         | 多环境静态机器人   | WD + CE + Mobile      | 环境多样性是否重要？   |
| **no CE**         | 跨实体数据(π0)     | WD + ME + Mobile      | 跨机器人迁移是否有效？ |
| **no ME or CE**   | 所有其他机器人数据 | WD + Mobile (仅400h)  | 只用直接数据够不够？   |

##### 评估方法

实验设计了两种评估条件：

- **Full Cleaning Tasks**（完整清理任务）：把盘子放进水槽、清理卧室地板等复杂长时的真实场景任务
- **OOD Evaluation**（分布外评估）：把指定物体放进抽屉，难点在于这些物体可能是训练时从未见过的新类别

对于这两种评估，研究者使用了两个指标来衡量性能。**Success Rate**（成功率）衡量子任务的平均完成百分比，例如需要移动5个物体成功移动4个就是80%。**Language Following Rate**（语言遵循率）则衡量机器人行为与用户指令的相符程度。两个指标的区别在于：机器人可能做了某些事情但做错了方向，此时成功率低但可能部分遵循了指令。

##### 实验结果分析

![alt text](<VLA Learning Notes.assets/image.png>)

| 场景                | 主要发现                 | 说明                           |
| ------------------- | ------------------------ | ------------------------------ |
| **In-Distribution** | 去掉ME或CE后性能下降明显 | 其他机器人数据对基础性能很重要 |
| **OOD (新物体)**    | 去掉WD性能下降最严重     | Web数据帮助识别新物体类别      |
| **所有场景**        | ME和CE都重要             | 物理技能迁移在所有情况下都有用 |

**关键发现**：**Web数据 (WD)** 对OOD场景（新物体）特别重要，原因是教会模型物体的语义（"什么是玩具"、"什么是盘子"），没有WD连新物体都认不出来。**其他机器人数据 (ME + CE)** 对所有场景都重要，提供物理技能和环境多样性，即使embodiment不同，技能也能迁移。

##### 环境数量扩展性研究

![alt text](<VLA Learning Notes.assets/image-1.png>)

为了量化π0.5的泛化能力，研究者进行了一个扩展性研究：横轴是训练数据中不同环境的数量（从0到100+），纵轴是在新场景的语言遵循率（OOD Follow Rate）。黄色曲线展示π0.5在全新场景的表现，而绿色横线是一个"作弊"的Oracle baseline——直接在测试环境收集数据训练的模型，代表理论上限。

曲线呈现出明显的两个阶段：

- **快速上升阶段**（0→40个环境）：性能从14%暴涨到76%，每增加环境都带来大量新信息，学习最基本的环境多样性，边际效益最高
- **趋向稳定阶段**（40→100个环境）：从76%增长到87%，增速变慢但仍在提升，主要学习edge cases，逐渐接近理论上限

**关键发现**：约100个训练环境后，π0.5的性能接近甚至略超Oracle baseline。这说明通过在100个其他家庭训练，模型在全新家庭的表现能达到"直接在测试家庭训练"的水平。Oracle只有85%而非100%，反映了任务本身的客观难度、硬件限制等因素。π0.5能达到87%，充分证明了co-training策略的有效性。

**实际意义**：

- **数据需求可行**（relatively accessible）：不需要1000或10000个环境，100个家庭×4小时≈400小时mobile data就足够，对研究团队/公司是可实现的目标
- **真正的泛化**：模型学会了泛化规律而非记忆环境，与简单过拟合有本质区别
- **商业化路径清晰**：核心数据只需~100环境的mobile robot数据，配合web数据和其他机器人数据，通过co-training实现强泛化

##### 核心结论

这个实验证明了数据的分工明确（WD→语义理解和识别新物体，ME→环境多样性，CE→物理技能迁移），绝大部分（97.6%）数据来自非目标平台但通过co-training得到有效利用。**核心结论**：数据多样性 > 数据相关性，co-training + 适量直接数据 = 强泛化能力。

#### 模型训练与推理

##### 模型架构

![alt text](<VLA Learning Notes.assets/image-2.png>)

π0.5基于π0 VLA，通过co-training能够输出多种类型的标签（包括动作和文本），因此可以用**同一个模型**控制机器人的高层和低层。整个系统包含Pre-trained VLA和Action Expert（300M参数），虽然Action Expert是独立模块，但它是π0.5的内置组件，与VLA联合训练，通过内部表示直接传递信息，而非两个独立的模型系统。

##### 推理流程

首先是**输入**（左下）：多视角图像（3张卧室场景）+ 高层提示（"clean the bedroom"），经过**Pre-trained VLA编码**（蓝色token序列）处理视觉和语言输入。

**Step 1 - 高层推理**：**高层分支**（上方灰色区域）通过离散自回归token解码生成子任务预测。例如，给定任务"clean the bedroom"和当前观察图像，模型会生成语义层面的子任务预测，如"pick up the pillow"。这个过程本质上是让模型"告诉自己"下一步应该做什么，类似chain-of-thought推理。

**Step 2 - 低层推理**：**低层分支**（右侧黄色区域）将高层命令"pick up the pillow"与VLA内部表示一起输入Action Expert（300M参数），通过**Flow Matching**技术和绿色方块的中间表示，最终输出连续动作[-1.7, 1.3, 3.1, 1.4]。这些连续动作包括关节角度、速度等底层控制信号，形成50-step（1秒）的action chunk。

图中**关键的虚线箭头**表示高层决策如何传递给低层，以及Action Expert如何从内部表示生成连续动作。

##### 技术实现细节

| 路径                         | 用途     | 技术               | 输出                                         |
| ---------------------------- | -------- | ------------------ | -------------------------------------------- |
| **Discrete Auto-regressive** | 高层推理 | Token-by-token生成 | 文本形式的语义动作（如"pick up the pillow"） |
| **Flow Matching**            | 低层推理 | 连续序列生成       | 50步的连续动作（1秒@50Hz控制频率）           |

**Action Chunk机制**：采用50-step action chunking而非单步预测，原因包括控制频率合理（50Hz是常见的机器人控制频率）、减少模型调用（提高实时性）、动作更流畅（避免抖动）、可提前规划（考虑1秒内的短期未来）。

##### 关键创新点

这个方法借鉴了Hi Robot系统的分层思想，但关键区别在于π0.5使用**同一个模型**进行高层决策和低层控制。这带来的好处包括：

- **信息无损传递**：不只通过文本，还通过内部表示（hidden states）传递丰富信息
- **避免模型间磨合**：高层和低层共享对视觉、物体、物理的理解，不会出现"理解不一致"  
- **端到端优化**：可以根据最终任务成功率联合优化整个pipeline，高低层学会互相配合
- **统一的"世界观"**：同一个模型学到的概念是一致的，不会出现"高层说pick up，低层不知道是什么"的问题

---

##### 概念补充：回归、离散Token生成、扩散模型、流匹配

为了理解π0.5的技术选择（为什么Pre-training用FAST、Post-training用Flow Matching），这里补充四种主要动作生成方法的原理和技术细节。

###### 1. 回归 (Regression)

**核心思想**：学习映射函数 $f_\theta: X \to Y$ ，从观察（图像、状态）直接预测动作。神经网络通过多层非线性变换将输入映射到连续值输出。
  
**训练目标**：给定数据集 $D = \{(x_i, y_i)\}$ （观察-动作对），最小化预测值与真实值的差距：

$$
\min_\theta \sum_i L(f_\theta(x_i), y_i)
$$

常用均方误差（MSE）损失： $L(\hat{y}, y) = ||\hat{y} - y||^2$ ，表示预测动作 $\hat{y}$ 与真实动作 $y$ 的欧氏距离平方。通过梯度下降优化参数： $\theta_{t+1} = \theta_t - \eta \nabla_\theta L$ 。

**网络结构**：输入观察（如图像+机器人状态） → 多层全连接/卷积层 → 输出动作向量。最后一层直接输出连续值，无需激活函数（如sigmoid/softmax）。

**优势**：简单高效，推理只需一次前向传播（O(1)复杂度），适合实时控制。

**致命缺陷——多峰分布问题**：回归学习的是条件期望 $E[y|x]$ ，只能输出单一点估计。当同一观察对应多种合理动作时（如从左/右/上方抓杯子），回归输出所有可能性的平均值。

**问题示例**：训练数据中"抓杯子"有50%从左抓、50%从右抓。回归会输出平均值（从中间抓），导致机器人手臂撞到杯子！这是因为MSE的最优解是条件期望： $\arg\min_{\hat{y}} E[||y - \hat{y}||^2] = E[y|x]$ ，数学上必然输出平均值。

###### 2. 离散Token生成 (Autoregressive)

**核心思想**：将连续动作离散化为有限的token集合，用类似GPT的自回归模型逐个token生成。解决多峰分布问题：不同token序列可以表示不同的动作模式。

**自回归分解**：将动作序列 $y = [y_1, ..., y_n]$ 分解为条件概率的乘积：

$$
P(y) = P(y_1) \cdot P(y_2|y_1) \cdot P(y_3|y_1,y_2) \cdot \ldots \cdot P(y_n|y_1,...,y_{n-1})
$$

每个token的预测都依赖于前面所有token，这样可以捕捉序列内部的依赖关系。

**训练目标**：最大化真实token序列的对数似然，等价于最小化交叉熵损失：

$$
L = -\sum_i \log P(y_i | y_{1:i-1})
$$

**推理过程**：逐token采样生成。每步模型输出词汇表上的概率分布 $P(\cdot|context) = \text{softmax}(logits)$ ，从中采样下一个token。可用temperature参数 $T$ 控制多样性： $T$ 越高越随机， $T$ 越低越确定。

**离散化方法**：

- **简单分桶**：将连续值域`[-1, 1]`均匀划分为K个区间（如256档），`0.512` → token `131`
- **Vector Quantization (VQ)**：学习codebook $\{v_1, ..., v_K\}$ ，编码时映射到最近code向量： $\arg\min_i ||a - v_i||$
- **FAST**（π0预训练用）：利用时间相关性压缩，将50步×7维=350个连续值压缩为仅8个离散token，大幅减少序列长度

**优势**：能表示多峰分布（不同token序列=不同动作），训练稳定（与VLM架构统一）。

**局限性**：

- 离散化损失精度：`0.512` → `0.51`的量化误差在精细操作中累积
- 推理慢：自回归需要串行生成（FAST需8次模型调用），难以并行加速
- 不适合高频控制：对需要50Hz实时响应的机器人，8次串行调用太慢

###### 3. 扩散模型 (Diffusion Models)

[Bilibiili -【大白话01】一文理清 Diffusion Model 扩散模型 | 原理图解+公式推导](https://www.bilibili.com/video/BV1xih7ecEMb?vd_source=de5f41a81376d29995d1f5309edd3f52)

[笔记 - Denoising Diffusion Probabilistic Models Tutorial（扩散模型）](../Paper/Denoising%20Diffusion%20Probabilistic%20Models%20Tutorial（扩散模型）.pdf)

**核心思想**：通过学习逆转数据的加噪过程来生成样本。将原始数据逐步添加噪声，获得含有每一步添加了噪声的版本。然后训练模型学习逆转这个加噪过程，从而生成原始数据。直接预测上一步的图像是很困难的，因此我们训练模型预测每一步添加的噪声，从而可以一步步去噪得到原始数据。

**前向过程（加噪）**：给真实动作 $x_0$ 逐步加高斯噪声，经过T步（如1000步）变成纯噪声 $x_T$ 。关键性质是可以一步到位采样任意时刻的噪声版本：

$$
x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1-\bar{\alpha}_t} \cdot \epsilon,\quad \epsilon \sim \mathcal{N}(0,I)
$$

其中 $\bar{\alpha}_t$ 控制噪声程度（ $t$ 越大噪声越多）。

**训练目标**：训练神经网络 $\epsilon_\theta(x_t, t)$ 预测每一步加入的噪声，最小化预测误差：

$$
L = E_{t,x_0,\epsilon} [||\epsilon - \epsilon_\theta(x_t, t)||^2]
$$

训练时：随机选时间步 $t$ ，从数据集采样动作 $x_0$ ，生成加噪版本 $x_t$ ，让模型预测噪声 $\epsilon$ 。

**反向过程（去噪/推理）**：从纯噪声 $x_T \sim \mathcal{N}(0,I)$ 开始，每步用模型预测噪声并去除，迭代T步得到最终动作。DDPM的去噪公式：

$$
x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z
$$

其中 $z$ 是少量随机性（除最后一步）。需要T=50-1000次迭代，每次调用一次模型。

**优势**：能建模复杂多峰分布，生成质量高。**劣势**：推理慢（需要几百上千次模型调用），不适合需要实时响应的机器人控制。

**详细内容请见：[笔记 - Denoising Diffusion Probabilistic Models Tutorial（扩散模型）](https://share.goodnotes.com/s/9DPIOpPEblnTRYqkrWZxiG)**

###### 4. 流匹配 (Flow Matching)

**核心思想**：学习时间依赖的向量场 $v_\theta(x, t)$ ，描述如何从简单分布（标准高斯噪声）流动到目标分布（真实动作）。类比GPS导航：学习从任意起点到终点的"最优导航路径"。

**定义确定性路径**：从噪声 $x_0 \sim \mathcal{N}(0,I)$ 到真实动作 $x_1$ 之间建立线性插值路径：

$$
x_t = (1-t) \cdot x_0 + t \cdot x_1, \quad t \in [0,1]
$$

这条路径的"速度场"（切向量）为 $v_t(x) = dx/dt = x_1 - x_0$ ，表示在路径上每个点应该往哪个方向移动。

**训练目标**：训练神经网络 $v_\theta(x_t, t)$ 学习预测速度场，最小化与真实速度的差距：

$$
L = E_{t,x_0,x_1} [||v_\theta(x_t, t) - (x_1 - x_0)||^2]
$$

训练时：随机采样时间 $t \in [0,1]$ 、噪声 $x_0$ 和真实数据 $x_1$ ，计算插值点 $x_t$ ，让模型预测应该往哪个方向走。

**推理过程**：使用欧拉法数值积分求解ODE（常微分方程）：

$$
x_{i+1} = x_i + v_\theta(x_i, t_i) \cdot \Delta t
$$

从噪声 $x_0$ 开始，每步询问模型"当前应该往哪走"，迭代K=5-10步即可到达目标动作。相比Diffusion基于SDE（随机微分方程），Flow Matching的路径是确定性的，更稳定可控。

**核心优势**：

- **推理效率高**：5-10步 vs Diffusion的50-1000步，减少90-99%计算
- **训练稳定**：目标速度场 $(x_1 - x_0)$ 有解析形式，无需设计复杂的noise schedule
- **确定性路径**：基于ODE而非SDE，轨迹可预测可控

**详细内容请见：[笔记 - Flow Matching Explained - From Noise to Robot Actions（流匹配）](../Paper/Flow%20Matching%20Explained%20-%20From%20Noise%20to%20Robot%20Actions（流匹配）.pdf)**

**π0.5的应用**：Action Expert用Flow Matching生成50步×7维的连续动作chunk（1秒@50Hz控制频率）。推理时约10步迭代（~200ms），满足实时控制需求。这是π0.5选择Flow Matching而非Diffusion的关键原因。

| 方法          | 训练目标      | 推理复杂度      | 表达能力 | 精度             |
| ------------- | ------------- | --------------- | -------- | ---------------- |
| 回归          | MSE           | O(1)            | 单峰     | 高               |
| 离散Token     | Cross-entropy | O(n) 自回归     | 多峰     | 中（离散化损失） |
| Diffusion     | 预测噪声      | O(T), T=50-1000 | 多峰     | 高               |
| Flow Matching | 预测速度场    | O(K), K=5-10    | 多峰     | 高               |

π0.5采用混合策略：Pre-training阶段使用离散Token（FAST）以提高训练效率和稳定性，Post-training阶段引入Flow Matching以实现高精度、低延迟的实时控制。这样既利用了离散token训练快、易与VLM统一的优势，又通过Flow Matching获得连续动作的高精度和实时性。

---

#### 实际应用和未来改进

π0.5的关键测试是让机器人在从未见过的新家庭环境中执行复杂的长时清洁任务（如收拾盘子、铺床、清理卧室）。这比之前的VLA演示更困难，因为过去的演示通常在与训练数据相同或相似的环境中进行。实验结果显示π0.5能够：处理环境变化和人类干扰、接受不同粒度的语言指令（从高层的"put the dishes in the sink"到具体的"pick up the round brush"）、完成需要语义理解和子任务分解的复杂行为。

虽然π0.5仍有局限（在高层语义推断和电机命令方面会犯错），但这证明了通过co-training让机器人从多种知识源学习的方法是有效的。未来的改进方向包括：利用verbal feedback、从自主经验学习、在不熟悉情况下主动寻求帮助，以及改进知识迁移和增加数据源多样性。

### 1.2 Paper-Pi05: a Vision-Language-Action Model with Open-World Generalization

[Paper arxiv](https://arxiv.org/abs/2504.16054)

[Local PDF](../Paper/Paper-Pi05:%20a%20Vision-Language-Action%20Model%20with%20Open-World%20Generalization.pdf)

#### Abstract核心要点

**目前问题**：虽然VLA模型在端到端机器人控制上表现优异，但**开放世界泛化** (open-world generalization) 仍然是未解决的核心挑战。

**解决方案**：π0.5基于π0，使用**异构任务联合训练** (co-training) 实现广泛泛化。关键在于"**异构数据** (heterogeneous data)" - 来自不同类型、不同来源的任务数据。

**技术路线**：

- **多源数据融合**：多机器人数据 + 高层语义预测 + 网络数据等
- **混合多模态样本**：图像观察 + 语言指令 + 物体检测 + 语义子任务预测 + 底层动作  
- **知识迁移验证**：证明知识迁移对有效泛化的重要性

**突破性成果**：**首次实现**端到端学习驱动的机器人系统在全新家庭环境中执行长时程精细操作技能（如厨房和卧室清理）。

![alt text](<VLA Learning Notes.assets/image-3.png>)

#### Introduction要点解析

##### 问题本质：从"能力不足"到"泛化不足"

当前机器人的根本问题不是技术能力（敏捷性、灵活性），而是**泛化能力**。大多数VLA模型在实验室表现优秀，但一到真实世界就失效。

**重要观点**：机器人需要的不是更强的单项能力，而是在**多个抽象层面**同时泛化的能力。

##### 泛化的三个层次（以厨房清理为例）

| 泛化层次     | 难度 | 例子                               | 解决方案                 |
| ------------ | ---- | ---------------------------------- | ------------------------ |
| **感知泛化** | 低   | 识别不同外观的盘子、刀具           | 足够的视觉多样性         |
| **技能泛化** | 中   | 用新的方式或序列组合已有动作       | 跨环境、跨任务的技能迁移 |
| **语义泛化** | 高   | 理解"哪个是晾衣架"、"该开哪个抽屉" | 常识知识和推理能力       |

**关键发现**：不同层次的泛化需要**不同类型的数据源**，无法通过单一数据类型解决。

##### 人类学习的启发：多源知识融合

人类解决新问题的能力来自**知识的组合运用**：

- **间接知识**：书本、他人经验 → 对应网络数据的语义知识
- **类比迁移**：其他任务的经验 → 对应跨机器人平台的技能迁移  
- **直接经验**：目标任务练习 → 对应目标平台的直接数据

**重要发现**：**知识多样性 > 数据相关性**。97.6%的"不相关"数据通过合理配方能产生强泛化。

##### VLA的技术突破：统一建模框架

传统方法的问题：异构数据源无法有效整合，需要设计复杂的多模型系统。

**VLA的突破**：**序列建模框架** (sequence modeling) 将所有模态（视觉、语言、动作）统一为token序列，使异构数据的联合训练成为可能。

**技术优势**：

1. **统一表示**：不同数据类型可以在同一模型中处理
2. **端到端优化**：避免多模型系统的累积误差
3. **知识共享**：不同数据源的知识可以相互促进

**设计哲学转变**：从"收集更多直接数据"到"设计更好的数据配方"。

#### Related Work：π0.5的技术定位

##### 现有工作的进展与局限

**通用机器人操作策略的发展**：

近期工作（BridgeData V2、Open X-Embodiment等）证明将训练数据从单任务扩展到多场景、多任务的多样化数据集能显著提高泛化能力。VLA模型（如RT-2、OpenVLA）通过微调预训练视觉-语言模型，能够利用网络规模预训练获得的语义知识，结合高表达力的动作解码机制（flow matching、diffusion、高级action tokenization），在真实世界执行复杂操作任务。

**根本局限**：尽管语言遵循能力impressive，现有VLA仍然主要在**与训练数据密切匹配的环境**中评估。即使一些研究表明简单技能（如抓取物体、开抽屉）可以通过在更广泛环境中收集数据来泛化，但将同样方法应用到更复杂的长时程任务（如清理厨房）仍然困难，通过暴力扩展收集数据来覆盖所有可能场景是不可行的。

##### π0.5的技术突破点

| 技术方向                 | 现有方法局限                       | π0.5的创新                                       |
| ------------------------ | ---------------------------------- | ------------------------------------------------ |
| **非机器人数据联合训练** | 主要局限于VLM数据混合              | 设计更广泛的机器人相关监督源联合训练系统         |
| **高层推理与规划**       | 使用两个独立模型（VLM + 低层策略） | **同一模型**进行高低层推理，类似chain-of-thought |
| **开放世界泛化**         | 限制在基本原语，任务相对简单       | 长时多阶段任务（10-15分钟），在全新家庭环境      |
| **数据利用策略**         | 主要依赖目标任务相关数据           | 97.6%"不相关"数据通过co-training实现强泛化       |

##### 非机器人数据联合训练的探索

许多先前工作尝试使用多样化的非机器人数据来改进机器人策略的泛化：

- **视觉编码器初始化**：从计算机视觉数据集初始化视觉编码器
- **VLM数据混合**：RT-2等展示了与VLM训练数据联合训练能改善泛化，特别是与新物体或未见过的场景背景交互时
- **现成规划器**：利用现成的任务规划器辅助机器人控制

**π0.5的超越**：超越了VLM数据联合训练，设计了一个系统来与**更广泛的机器人相关监督源**进行联合训练，包括其他机器人数据、高层语义子任务预测、语言指导演示等。实验表明这种特定的数据源组合能够让移动机器人在全新环境中执行复杂长时程行为。

##### 高层推理与语言规划

许多工作表明用高层推理增强端到端策略可以显著改善长时程任务性能，特别是当高层子任务推理可以利用大型预训练LLM和VLM时。

**现有方法**：通常使用**两个独立模型** - VLM预测语义步骤，独立的低层策略执行这些步骤。

**π0.5的方法**：也使用两阶段推理（首先推理高层语义子任务，然后基于此预测动作），但使用**同一个模型**进行高层和低层推理，配方更接近chain-of-thought或test-time compute方法。关键区别是高层推理仍然以低于低层动作推理的频率运行。

**技术优势**：统一模型使得高低层推理能够共享内部表示，不仅通过文本还通过hidden states传递信息，避免了两模型系统的理解不一致和优化困难问题。

##### 开放世界泛化的评估

**现有工作的局限**：

大多数机器人学习系统在与训练数据密切匹配的环境中评估。当机器人任务限制在更窄的基本原语集合（如抓取）时，一些允许任务特定假设的方法（如grasp prediction、model-based planning and control）已经显示能够广泛泛化甚至到全新家庭，但这些方法不容易泛化到通用机器人可能需要执行的全部任务范围。

近期大规模数据集工作显示简单的端到端学习任务能够泛化到新环境，但这些演示中的任务仍然相对简单，通常少于1分钟长度，成功率往往较低。

**π0.5的突破**：

展示π0.5能够执行长时多阶段任务，如把所有盘子放进水槽或把所有衣物从新卧室地板上拾起，同时泛化到**完全新的家庭环境**（entirely new homes）。这是首次端到端学习驱动的机器人系统在全新环境中执行如此复杂和长时程的操作技能（10-15分钟）。

#### Preliminaries：VLA技术基础

##### VLA的训练目标

VLA模型通过模仿学习训练，目标是最大化给定观察和语言指令下，动作序列的对数似然：

$$
\max_\theta \mathbb{E}_{(a_{t:t+H}, o_t, \ell) \sim D} \left[ \log \pi_\theta(a_{t:t+H} | o_t, \ell) \right]
$$

**公式含义**：

- **θ**：模型参数（神经网络的权重），这是训练的目标。
- **D**：机器人演示数据集，包含以下的三元组的数据。
- **数据三元组（下标）**：
  - $a_{t:t+H}$：动作序列（action chunk），从时刻t到t+H这一段时间内的连续动作（如50步×7维=350个数值）。
  - $o_t$：时刻t的观察（由对环境的外部感知以及对自身状态的内部感知组成，详见[观察的组成结构](#观察的组成结构)）
  - $ℓ$：语言指令
- **期望 E**：对所有训练数据求平均
- **log π_θ(...)**：模型预测该动作序列的对数概率。取对数在保持优化目标不变的同时把乘法变成加法，避免数值计算问题。

**公式整体含义**：我们想要训练一组模型的参数θ，使得模型对于给定数据集D中的所有三元组（动作序列、观察、语言指令），能根据当前的观察和语言指令，预测出正确动作序列的对数概率的平均值最大化。

**训练过程**：通过调整参数θ，让模型在所有训练数据上预测正确动作序列的平均对数概率最大化。实际训练时用随机梯度下降，每次采样一小批数据计算loss并更新参数。

##### 观察的组成结构

观察 $o_t$ 不是单一数据，而是**外部感知 + 内部感知**的组合：

###### 1. 外部感知 (Exteroception)

多视角图像 $I_1^t, ..., I_n^t$：

- 通常3个摄像头（手腕视角、头部视角、外部视角，具体数量根据具体任务和机器人平台而定）
- 每张图像如224×224×3的RGB图像
- 提供环境、物体位置等信息

###### 2. 内部感知 (Proprioception)

本体感知状态 $q_t$：

- **关节角度**：每个关节当前的角度值
- **夹爪状态**：开合程度（0到1）
- **底盘状态**（移动机器人）：位置、朝向、速度

**为什么需要两者结合？**

- 只有图像：看到物体但不知道自己手臂在哪
- 只有本体感知：知道姿势但不知道周围环境
- 两者结合：能够规划"如何从当前姿势到达目标"

##### VLA架构设计

**基本架构**：遵循现代视觉-语言模型设计

| 组件                   | 作用                       | 技术                            |
| ---------------------- | -------------------------- | ------------------------------- |
| **模态特定tokenizers** | 将不同模态转成token表示    | 图像patches、文本分词、动作编码 |
| **自回归transformer**  | 从输入tokens生成输出tokens | 标准transformer架构             |
| **权重初始化**         | 从预训练VLM初始化          | 利用网络规模的语义知识          |

**统一表示机制**：

不同模态的数据（图像、文本、动作）含义完全不同，无法直接拼接。解决方案是通过**Embedding层**将所有模态转成相同维度的向量：

| 模态     | 原始格式          | Encoder           | 统一表示  |
| -------- | ----------------- | ----------------- | --------- |
| 文本     | "pick" → ID 1024  | Embedding Matrix  | 768维向量 |
| 图像     | 16×16×3 像素patch | Linear Projection | 768维向量 |
| 本体感知 | [θ1, θ2, ..., θ7] | Linear Layer      | 768维向量 |
| 动作     | 离散ID或连续值    | Embedding/Linear  | 768维向量 |

**统一到相同维度后**，所有tokens可以拼接成一个序列输入transformer，通过Attention机制互相交互。

**自回归生成**：模型逐个token生成输出，每次生成依赖前面已生成的tokens，类似GPT生成文本的方式。

##### 动作表示的两种方法

**为什么需要特殊的动作表示？**

动作是连续值（如关节角度0.523 rad），与离散的文本token不同。需要特殊方法将动作转成token或向量表示。

| 方法                     | 表示形式      | 训练         | 推理             | 精度             |
| ------------------------ | ------------- | ------------ | ---------------- | ---------------- |
| **离散化 (FAST)**        | 8个离散tokens | 快速、稳定   | 自回归解码（慢） | 中（有量化误差） |
| **连续 (Flow Matching)** | 350维连续向量 | 需要迭代训练 | 迭代去噪（10步） | 高               |

**π0.5的策略**：

- **Pre-training**：使用FAST离散tokens（训练效率高，和文本统一处理）
- **Post-training**：切换到Flow Matching（精度高，适合实时控制）

##### FAST编码原理

**问题**：50步×7维 = 350个连续值，如何高效编码？

**FAST的解决方案**：利用**时间相关性**压缩

**时间相关性**：机器人动作是平滑的，相邻时刻的动作非常相似。就像视频压缩利用相邻帧的相似性，FAST利用相邻时刻动作的相似性。

**压缩类比**：动作的"关键帧"

```text
原始: 50步完整动作序列（350个数）
  ↓ Encoder
8个关键"模式"（8个离散tokens）
  ↓ Decoder  
重建: 50步完整动作序列
```

**技术实现**：Vector Quantization (VQ)

1. **学习Codebook（动作模式库）**：包含256个codes，每个code是一个抽象的动作特征向量
2. **编码**：将350维动作映射到8个code IDs
3. **解码**：从8个codes通过神经网络decoder重建350维动作

**Code的本质**：

类似Language Model中的token：

- 离散的符号（ID）
- 对应抽象的向量表示
- 含义通过神经网络学习
- 不是人类可直接解释的"指令"，而是网络内部的抽象编码

**压缩效果**：350个数 → 8个tokens，压缩比约44倍

| 优势                | 劣势                      |
| :------------------ | :------------------------ |
| 序列短，训练快速    | 有量化误差                |
| 和文本token统一处理 | 推理需要8步自回归（较慢） |

##### Flow Matching for 连续动作（在Post-training阶段使用）

**核心思想**：学习从噪声到真实动作的"流动路径"

**数学表示**：

从噪声 $x_0 \sim \mathcal{N}(0,I)$ 到真实动作 $x_1 = a_{t:t+H}$ 建立线性插值路径：

$$
x_\tau = (1-\tau) \cdot x_0 + \tau \cdot x_1, \quad \tau \in [0,1]
$$

路径的速度场（方向）：

$$
v_\tau = \frac{dx}{d\tau} = x_1 - x_0
$$

**训练目标**：

训练神经网络 $v_\theta(x, \tau)$ 预测速度场：

$$
L = \mathbb{E}_{\tau, x_0, x_1} \left[ ||v_\theta(x_\tau, \tau) - (x_1 - x_0)||^2 \right]
$$

**推理过程（生成动作）**：

从纯噪声开始，迭代更新：

$$
x_{i+1} = x_i + v_\theta(x_i, \tau_i) \cdot \Delta\tau
$$

```text
Step 0: x = 随机噪声
Step 1: x = x + v_θ(x, 0.0) × 0.1
Step 2: x = x + v_θ(x, 0.1) × 0.1
...
Step 10: x = 最终清晰的动作
```

**只需10步迭代即可生成高质量动作，比Diffusion Model的50-1000步快得多！**

**Flow Matching vs Diffusion**：

| 特性     | Diffusion   | Flow Matching          |
| -------- | ----------- | ---------------------- |
| 路径类型 | 随机（SDE） | 确定性（ODE）          |
| 训练目标 | 预测噪声    | 预测速度场             |
| 推理步数 | 50-1000步   | 5-10步                 |
| 实时性   | 慢          | 快，适合50Hz机器人控制 |

**π0.5选择Flow Matching的原因**：推理快速，满足实时控制需求。

关于Diffusion Model和Flow Matching的更多细节，请参考：

**[笔记 - Denoising Diffusion Probabilistic Models Tutorial（扩散模型）](../Paper/Denoising%20Diffusion%20Probabilistic%20Models%20Tutorial（扩散模型）.pdf)**

**[笔记 - Flow Matching Explained - From Noise to Robot Actions（流匹配）](../Paper/Flow%20Matching%20Explained%20-%20From%20Noise%20to%20Robot%20Actions（流匹配）.pdf)**

##### Action Expert设计

**什么是Action Expert？**

一个专门的小型transformer（300M参数），负责Flow Matching的动作生成。

**架构设计**：

```text
输入tokens → 主Transformer (2B参数)
              ↓
      ┌──────┴──────┐
      ↓              ↓
   文本tokens    动作tokens
      ↓              ↓
  主模型处理    Action Expert处理
      ↓              ↓
   文本输出      连续动作输出
```

**Action Expert的输入**：

1. 主transformer的内部表示（语义理解）
2. 当前的部分去噪动作 $x_\tau$
3. 时间步 $\tau$

**Action Expert的输出**：

- 速度场 $v_\theta(x_\tau, \tau)$

**为什么需要单独的Action Expert？**

1. **专业化**：主模型理解语义，Action Expert专注精确动作生成
2. **效率**：Action Expert较小（300M vs 2B），Flow Matching需要迭代10次更经济
3. **模块化**：Pre-training不需要，Post-training时才加入

**类比Mixture of Experts (MoE)**：不同类型的tokens由不同的"专家"模块处理。

##### Pre-training vs Post-training对比

| 阶段              | 动作表示          | 模型组件            | 训练特点   | 目的                       |
| ----------------- | ----------------- | ------------------- | ---------- | -------------------------- |
| **Pre-training**  | FAST离散tokens    | VLM + FAST Decoder  | 快速、稳定 | 学习异构数据，建立基础能力 |
| **Post-training** | Flow Matching连续 | VLM + Action Expert | 精确、实时 | 专门优化目标任务的精确控制 |

**设计哲学**：结合两者优势，Pre-training用离散表示快速学习，Post-training用连续表示实现精确控制。

### 1.3 Code-openpi

[Github Repo](https://github.com/Physical-Intelligence/openpi)

[Local Repo](../Repo/openpi)

## 2. VLA Survey

### 2.1 A Survey on Vision-Language-Action Models: An Action Tokenization Perspective

[Paper arxiv](https://arxiv.org/abs/2507.01925)

[Local PDF](../Paper/A%20Survey%20on%20Vision-Language-Action%20Models:%20An%20Action%20Tokenization%20Perspective.pdf)

### 2.2 Pure Vision Language Action (VLA) Models: A Comprehensive Survey

[Paper arxiv](https://arxiv.org/abs/2509.19012)

[Local PDF](../Paper/Pure%20Vision%20Language%20Action%20(VLA)%20Models-%20A%20Comprehensive%20Survey.pdf)

## 3. MuJoco

[Github Repo](https://github.com/unitreerobotics/unitree_mujoco)

[Local Repo](../Repo/unitree_mujoco)
